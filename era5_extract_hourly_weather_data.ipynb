{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview\n",
    "# ERA 5 documentation: https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336421ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from pyproj import Transformer\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration & parameters\n",
    "WEB_MERCATOR_EXTENT = [\n",
    "    -10705956.3396000005304813,  # x_min (west)\n",
    "    290328.920800000021699,      # y_min (south) \n",
    "    -6626467.69309999998003244,  # x_max (east)\n",
    "    2842131.11159999994561108    # y_max (north)\n",
    "]\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR_GRIB = \"era5_data\"\n",
    "OUTPUT_DIR_GEOTIFF = \"era5_geotiffs\"\n",
    "\n",
    "# Hurricane timeline\n",
    "DATES = ['2020-10-31', '2020-11-01', '2020-11-02', '2020-11-03', '2020-11-04', '2020-11-05', '2020-11-06', '2020-11-07', '2020-11-08']\n",
    "TIMES = [f'{h:02d}:00' for h in range(24)]  # Hourly data\n",
    "\n",
    "# Variables to download\n",
    "VARIABLES_CONFIG = {\n",
    "    'core_variables': [\n",
    "        '10m_u_component_of_wind',\n",
    "        '10m_v_component_of_wind', \n",
    "        'mean_sea_level_pressure',\n",
    "        'total_precipitation',\n",
    "        '2m_temperature'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"   Dates: {DATES[0]} to {DATES[-1]}\")\n",
    "print(f\"   Variables: {len(VARIABLES_CONFIG['core_variables'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def convert_extent_to_latlon(web_mercator_extent):\n",
    "    \"\"\"Convert Web Mercator extent to WGS84 lat/lon for ERA5 API\"\"\"\n",
    "    x_min, y_min, x_max, y_max = web_mercator_extent\n",
    "    \n",
    "    # Create transformer from Web Mercator to WGS84\n",
    "    transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "    \n",
    "    # Transform corners\n",
    "    lon_min, lat_min = transformer.transform(x_min, y_min)\n",
    "    lon_max, lat_max = transformer.transform(x_max, y_max)\n",
    "    \n",
    "    # ERA5 API expects [north, west, south, east]\n",
    "    return [lat_max, lon_min, lat_min, lon_max]\n",
    "\n",
    "# Test the conversion\n",
    "area = convert_extent_to_latlon(WEB_MERCATOR_EXTENT)\n",
    "print(f\"Download area (N,W,S,E): {area}\")\n",
    "print(f\"   Coverage: {area[0]:.2f}°N to {area[2]:.2f}°N, {area[1]:.2f}°W to {area[3]:.2f}°W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a81d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR_GRIB, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_GEOTIFF, exist_ok=True)\n",
    "\n",
    "print(f\"Created directories:\")\n",
    "print(f\"   {OUTPUT_DIR_GRIB}/ - for GRIB files\")\n",
    "print(f\"   {OUTPUT_DIR_GEOTIFF}/ - for GeoTIFF files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a748b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CDS API client (~/.cdsapirc needs to be configured)\n",
    "# content of ~/.cdsapirc file: \n",
    "# url: https://cds.climate.copernicus.eu/api/v2\n",
    "# key: YOUR_API_KEY_HERE\n",
    "try:\n",
    "    c = cdsapi.Client()\n",
    "    print(\"CDS API client initialized successfully!\")\n",
    "    print(\"   Make sure your ~/.cdsapirc file is configured with your API key\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing CDS API: {e}\")\n",
    "    print(\"   Please check your ~/.cdsapirc configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81818b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download variables\n",
    "def download_variable_set(var_set_name, variables, area, dates, times, output_dir):\n",
    "    \"\"\"Download a set of variables from ERA5\"\"\"\n",
    "    filename = f\"{output_dir}/era5_{var_set_name}.grib\"\n",
    "    \n",
    "    print(f\"Downloading {var_set_name}...\")\n",
    "    print(f\"   Variables: {variables}\")\n",
    "    print(f\"   Output: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': variables,\n",
    "                'date': dates,\n",
    "                'time': times,\n",
    "                'area': area,  # [north, west, south, east]\n",
    "                'format': 'grib',\n",
    "                'grid': [0.25, 0.25],  # 0.25 degree resolution\n",
    "            },\n",
    "            filename\n",
    "        )\n",
    "        print(f\"Downloaded {var_set_name} to {filename}\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {var_set_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download core variables\n",
    "core_file = download_variable_set(\n",
    "    'core_variables', \n",
    "    VARIABLES_CONFIG['core_variables'], \n",
    "    area, \n",
    "    DATES, \n",
    "    TIMES, \n",
    "    OUTPUT_DIR_GRIB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GRIB to GeoTIFF\n",
    "def convert_grib_to_geotiff(grib_file, output_dir):\n",
    "    \"\"\"Convert GRIB2 files to individual GeoTIFF files\"\"\"\n",
    "    \n",
    "    if not os.path.exists(grib_file):\n",
    "        print(f\"File not found: {grib_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Converting {grib_file} to GeoTIFF format...\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file, engine='cfgrib', backend_kwargs={'indexpath': ''})\n",
    "        \n",
    "        # Set spatial dimensions\n",
    "        ds = ds.rename({'longitude': 'x', 'latitude': 'y'})\n",
    "        ds = ds.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Convert each variable\n",
    "        converted_files = []\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"   Converting variable: {var_name}\")\n",
    "            \n",
    "            var_data = ds[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Save each time step separately\n",
    "                for i, time_val in enumerate(var_data.time.values):\n",
    "                    time_str = str(time_val)[:13].replace(':', '').replace('-', '').replace(' ', '_')\n",
    "                    \n",
    "                    output_file = f\"{output_dir}/{var_name}_{time_str}.tif\"\n",
    "                    \n",
    "                    # Select single time step\n",
    "                    time_slice = var_data.isel(time=i)\n",
    "                    \n",
    "                    # Save as GeoTIFF\n",
    "                    time_slice.rio.to_raster(output_file, compress='lzw')\n",
    "                    converted_files.append(output_file)\n",
    "                    \n",
    "            else:\n",
    "                # Single time step\n",
    "                output_file = f\"{output_dir}/{var_name}.tif\"\n",
    "                var_data.rio.to_raster(output_file, compress='lzw')\n",
    "                converted_files.append(output_file)\n",
    "        \n",
    "        print(f\"Converted {len(converted_files)} files from {grib_file}\")\n",
    "        return converted_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {grib_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Conversion function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables\n",
    "if core_file and os.path.exists(core_file):\n",
    "    core_tiffs = convert_grib_to_geotiff(core_file, OUTPUT_DIR_GEOTIFF)\n",
    "else:\n",
    "    print(\"Core variables GRIB file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wind speed magnitude\n",
    "def create_wind_speed_magnitude(output_dir):\n",
    "    \"\"\"Calculate wind speed magnitude from u and v components\"\"\"\n",
    "    print(\"Calculating wind speed magnitude...\")\n",
    "    \n",
    "    try:\n",
    "        # Look for the ACTUAL filenames that were created\n",
    "        u_files = sorted(glob.glob(f\"{output_dir}/u10_*.tif\"))  # Changed from 10m_u_component_of_wind\n",
    "        v_files = sorted(glob.glob(f\"{output_dir}/v10_*.tif\"))  # Changed from 10m_v_component_of_wind\n",
    "        \n",
    "        print(f\"   Found {len(u_files)} u-component files\")\n",
    "        print(f\"   Found {len(v_files)} v-component files\")\n",
    "        \n",
    "        if len(u_files) == 0:\n",
    "            print(\"No u-component files found. Checking available files...\")\n",
    "            all_files = glob.glob(f\"{output_dir}/*.tif\")\n",
    "            wind_files = [f for f in all_files if 'u10' in f or 'v10' in f]\n",
    "            print(f\"   Available wind files sample: {wind_files[:3]}\")\n",
    "            return []\n",
    "        \n",
    "        wind_speed_files = []\n",
    "        for u_file, v_file in zip(u_files, v_files):\n",
    "            # Extract timestamp from filename\n",
    "            timestamp = u_file.split('_')[-1].replace('.tif', '')\n",
    "            \n",
    "            # Open u and v components\n",
    "            u_data = rioxarray.open_rasterio(u_file)\n",
    "            v_data = rioxarray.open_rasterio(v_file)\n",
    "            \n",
    "            # Calculate wind speed magnitude\n",
    "            wind_speed = (u_data**2 + v_data**2)**0.5\n",
    "            \n",
    "            # Save wind speed\n",
    "            output_file = f\"{output_dir}/wind_speed_{timestamp}.tif\"  # Simplified name\n",
    "            wind_speed.rio.to_raster(output_file, compress='lzw')\n",
    "            wind_speed_files.append(output_file)\n",
    "        \n",
    "        print(f\"Created {len(wind_speed_files)} wind speed magnitude files\")\n",
    "        return wind_speed_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating wind speed files: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the corrected wind speed calculation\n",
    "wind_speed_files = create_wind_speed_magnitude(OUTPUT_DIR_GEOTIFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract precipitation hours\n",
    "def extract_all_precipitation_hours(grib_file, output_dir):\n",
    "    \"\"\"Extract ALL precipitation time steps (every hour)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        ds_precip = xr.open_dataset(\n",
    "            grib_file, \n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={\n",
    "                'filter_by_keys': {'shortName': 'tp'},\n",
    "                'indexpath': ''\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        var_data = ds_precip['tp']\n",
    "        print(f\"   Full shape: {var_data.shape}\")\n",
    "        print(f\"   Dimensions: {var_data.dims}\")\n",
    "        print(f\"   Time points: {len(var_data.time)}\")\n",
    "        print(f\"   Step points: {len(var_data.step)}\")\n",
    "        \n",
    "        # Set spatial dimensions\n",
    "        ds_precip = ds_precip.rename({'longitude': 'x', 'latitude': 'y'})\n",
    "        ds_precip = ds_precip.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "        ds_precip = ds_precip.rio.write_crs(\"EPSG:4326\")\n",
    "        var_data = ds_precip['tp']\n",
    "        \n",
    "        precip_files = []\n",
    "        total_combinations = len(var_data.time) * len(var_data.step)\n",
    "        \n",
    "        print(f\"   Processing {total_combinations} time-step combinations...\")\n",
    "        \n",
    "        # Process each time × step combination\n",
    "        for i, time_val in enumerate(var_data.time.values):\n",
    "            for j, step_val in enumerate(var_data.step.values):\n",
    "                \n",
    "                # Parse time and step\n",
    "                base_time = pd.to_datetime(time_val)\n",
    "                step_hours = int(step_val.astype('timedelta64[h]') / pd.Timedelta('1 hour'))\n",
    "                \n",
    "                # Calculate actual time (base_time + step)\n",
    "                actual_time = base_time + pd.Timedelta(hours=step_hours)\n",
    "                \n",
    "                # Create timestamp string\n",
    "                time_str = actual_time.strftime('%Y%m%dT%H')\n",
    "                \n",
    "                # Select this specific time and step\n",
    "                time_step_slice = var_data.isel(time=i, step=j)\n",
    "                \n",
    "                # Save as single-band GeoTIFF\n",
    "                output_file = f\"{output_dir}/total_precipitation_{time_str}.tif\"\n",
    "                time_step_slice.rio.to_raster(output_file, compress='lzw')\n",
    "                precip_files.append(output_file)\n",
    "                \n",
    "                # Progress update\n",
    "                file_num = i * len(var_data.step) + j + 1\n",
    "                if file_num % 24 == 0 or file_num == total_combinations:\n",
    "                    print(f\"     ✓ Processed {file_num}/{total_combinations} files\")\n",
    "        \n",
    "        print(f\"Extracted {len(precip_files)} hourly precipitation files\")\n",
    "        \n",
    "        # Remove old files (the 23 incomplete ones)\n",
    "        old_files = glob.glob(f\"{output_dir}/total_precipitation_*T*.tif\")\n",
    "        old_files = [f for f in old_files if f not in precip_files]\n",
    "        for old_file in old_files:\n",
    "            try:\n",
    "                os.remove(old_file)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return sorted(precip_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the corrected extraction\n",
    "all_precip_files = extract_all_precipitation_hours(\n",
    "    \"era5_data/era5_core_variables.grib\", \n",
    "    OUTPUT_DIR_GEOTIFF\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12292040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output files\n",
    "# List all generated files\n",
    "print(\"Generated Files Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GRIB files\n",
    "grib_files = glob.glob(f\"{OUTPUT_DIR_GRIB}/*.grib\")\n",
    "print(f\"\\nGRIB files ({len(grib_files)}):\")\n",
    "for f in grib_files:\n",
    "    size_mb = os.path.getsize(f) / (1024*1024)\n",
    "    print(f\"   {os.path.basename(f)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Check GeoTIFF files by variable\n",
    "tiff_files = glob.glob(f\"{OUTPUT_DIR_GEOTIFF}/*.tif\")\n",
    "print(f\"\\nGeoTIFF files ({len(tiff_files)}):\")\n",
    "\n",
    "# Group by variable\n",
    "variables_found = {}\n",
    "for f in tiff_files:\n",
    "    var_name = '_'.join(os.path.basename(f).split('_')[:-1])\n",
    "    if var_name not in variables_found:\n",
    "        variables_found[var_name] = 0\n",
    "    variables_found[var_name] += 1\n",
    "\n",
    "for var, count in sorted(variables_found.items()):\n",
    "    print(f\"   {var}: {count} time steps\")\n",
    "\n",
    "print(f\"\\nProcessing complete! Files ready for GIS analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
